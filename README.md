# ğŸ“š RAG_FASTAPI â€“ Retrieval-Augmented Generation Chat Assistant

A lightweight Retrieval-Augmented Generation (RAG) web application using **Python**, **FastAPI**, **LangChain**, **ChromaDB**, and **OpenAI GPT-4** for intelligent question-answering on uploaded PDF documents.

## ğŸ”— Try it live: [https://rag-fastapi.vercel.app](https://rag-fastapi.vercel.app)
> âœ¨ New: Frontend session persistence and PDF-specific chat memory now live!

---

## ğŸ§± Tech Stack

- **Backend**: FastAPI Â· LangChain Â· OpenAI Â· ChromaDB
- **Frontend**: Vite + React
- **Vector Store**: Chroma
- **Deployment**:
  - Backend: Railway (Docker)
  - Frontend: Vercel (Vite React)
- **Language Support**: Python 3.10

---

## ğŸš€ Features

- âœ… Upload PDF documents
- âœ… Extract and embed content using OpenAI Embeddings
- âœ… Store and retrieve vectors using ChromaDB
- âœ… Ask questions in a modern, chat-style interface
- âœ… PDF-specific chat memory per file hash
- âœ… Session persistence via browser sessionStorage
- âœ… Fully integrated, cloud-deployed RAG pipeline

---

## ğŸ“‚ Project Structure

```
RAG_FASTAPI/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ temp_files/
â”‚   â”œâ”€â”€ chroma/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ runtime.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ ...
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## âš™ï¸ Backend Setup (Railway + Docker)

### ğŸ”§ Local Development

```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### ğŸš¢ Deployment on Railway (Docker)

**Dockerfile (backend/Dockerfile):**
```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY ./backend /app
RUN apt-get update && apt-get install -y build-essential cargo && \
    pip install --upgrade pip && \
    pip install -r requirements.txt
EXPOSE 10000
CMD ["uvicorn", "main:app", "--host=0.0.0.0", "--port=10000"]
```

**runtime.txt**:
```
python-3.10.13
```

**Railway Project Settings:**
```
Root Directory: /backend
Dockerfile Path: ./Dockerfile
Environment Variable:
OPENAI_API_KEY=your-openai-key
```

**Example Backend URL**:
```
https://ragfastapi-production.up.railway.app
```

---

## ğŸŒ Frontend Setup (Vercel)

### ğŸ›  Local Development

```bash
cd frontend
npm install
npm run dev
```

`.env` file:
```
VITE_BACKEND_URL=http://localhost:8000
```

### ğŸš€ Deploy on Vercel

1. Push repo to GitHub
2. Visit [vercel.com](https://vercel.com) â†’ New Project â†’ Import repo
3. Set **Environment Variable**:
```
VITE_BACKEND_URL=https://ragfastapi-production.up.railway.app
```
4. Deploy ğŸ‰

---

## ğŸ”„ Connecting Frontend to Backend

In `App.jsx`:
```js
const BACKEND_URL = import.meta.env.VITE_BACKEND_URL;

await axios.post(`${BACKEND_URL}/ask/`, {
  question: "...",
  file_hash: currentHash,
});
```

---

## ğŸ§ª Example API Call

**POST** `/ask`
```json
{
  "question": "What is the paper about?",
  "file_hash": "abc123..."
}
```
Response:
```json
{
  "answer": "This paper discusses...",
  "source_documents": [...]
}
```

---

## ğŸ§¼ .gitignore Highlights

```bash
__pycache__/
*.pyc
.env
backend/chroma/
backend/temp_files/
```

---

## ğŸ§  Credits

Built with ğŸ’¡ by [Dimple Khatri](https://github.com/dimplek0424)  
Uses: Python Â· LangChain Â· ChromaDB Â· FastAPI Â· Vercel Â· Railway

---

## ğŸ“ License

MIT License

